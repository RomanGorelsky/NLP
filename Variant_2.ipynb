{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3c84315669a443cf8fd89ffa076ed0e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ec0936c88654b189d5917f2d0e34990",
              "IPY_MODEL_6d2ba3858f8c44dbad440dbb25417872",
              "IPY_MODEL_96711424ca794c5b8e9c481a70b04339"
            ],
            "layout": "IPY_MODEL_f75fd6383d75475397d808d8fa6b3caa"
          }
        },
        "2ec0936c88654b189d5917f2d0e34990": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b07e8cb877d746959a20e8da8dbe0cb1",
            "placeholder": "​",
            "style": "IPY_MODEL_ad375fccdc134ea09858f56041d63a6f",
            "value": "Downloading builder script: "
          }
        },
        "6d2ba3858f8c44dbad440dbb25417872": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9c5c4b340e64528b52a667fd049cf0d",
            "max": 2472,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_899aca8b3c1f49fbb59835e73746b1fc",
            "value": 2472
          }
        },
        "96711424ca794c5b8e9c481a70b04339": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca8e3923afbd4d8495be017fc160996c",
            "placeholder": "​",
            "style": "IPY_MODEL_b930e7a81b3344c1afc31115336d9414",
            "value": " 6.33k/? [00:00&lt;00:00, 185kB/s]"
          }
        },
        "f75fd6383d75475397d808d8fa6b3caa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b07e8cb877d746959a20e8da8dbe0cb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad375fccdc134ea09858f56041d63a6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9c5c4b340e64528b52a667fd049cf0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "899aca8b3c1f49fbb59835e73746b1fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ca8e3923afbd4d8495be017fc160996c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b930e7a81b3344c1afc31115336d9414": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RomanGorelsky/NLP/blob/main/Variant_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The task was done by Gorelskii Roman, student of the bachelor program \"Data Science and Business Analytics\". All the comments are both written in english and in russian languages.\n",
        "\n",
        "## Задание было выполнено Горельским Романом, учеником образовательной программы бакалавриата \"Прикладной анализ данных\". Все комментарии написаны на английском и русском языках."
      ],
      "metadata": {
        "id": "EJgt73vBILKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Token classification на примере задачи NER (12 баллов)\n",
        "\n",
        "Это домашнее задание проходит в формате peer-review. Это означает, что его будут проверять ваши однокурсники. Поэтому пишите разборчивый код, добавляйте комментарии и пишите выводы после проделанной работы.\n",
        "\n",
        "Классификация токенов — задача, в которой для каждого отдельного токена или слова необходимо определить его тип, например, часть речи. В этом ноутбуке вам предстоит решить подвид задачи Token Classification, а именно NER или Named Entity Recognition. Вам необходимо для каждого слова определить, обозначает ли оно именованную сущность, например, имя человека, название места и тд."
      ],
      "metadata": {
        "id": "OP5iCNKzqzUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Установим необходимые библиотеки: ```datasets```, ```transformers``` и ```seqeval```."
      ],
      "metadata": {
        "id": "CISIyjsLq3bK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D0viZrfhqgyw",
        "outputId": "2188c03e-b64a-4151-e402-e656ef2d102d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/519.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.3/519.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "Successfully installed datasets-2.14.4 dill-0.3.7 huggingface-hub-0.16.4 multiprocess-0.70.15 xxhash-3.3.0\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m131.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, transformers\n",
            "Successfully installed safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=2e6303ba8f9fa5a011ced474ea9a7402a5cb83ab9a73b7bbd1c8d595e9ccb74d\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForTokenClassification\n",
        "    )\n",
        "from datasets import load_dataset, load_metric"
      ],
      "metadata": {
        "id": "sKuavd3rq7ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "WRgK1tYVq9_O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "918270b2-a89c-4bdc-c26f-336b0f04321f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Подготовка данных\n",
        "\n",
        "Давайте поближе познакомимся с тем, как хранятся датасеты для NER. В этом задании вам предстоит работать с conll2003. Подробнее о нем можно узнать по этой [ссылке](https://huggingface.co/datasets/conll2003).\n",
        "\n",
        "В качестве предобученной модели воспользуемся DistilBERT. Это уменьшенная версия обычного BERT."
      ],
      "metadata": {
        "id": "29tWdANmrBWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "batch_size = 64"
      ],
      "metadata": {
        "id": "nBwhstlyrC1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузим данные с помощью функции load_dataset."
      ],
      "metadata": {
        "id": "AYLMjfacrEwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = load_dataset(\"conll2003\")"
      ],
      "metadata": {
        "id": "wM61EEV8rF7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Наши данные состоят из следующих выборок:"
      ],
      "metadata": {
        "id": "keEwz30xrIoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets"
      ],
      "metadata": {
        "id": "3iOZCyb9rJ-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0f5d398-1c46-4ccf-a19c-2fab9c9cb3d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 14041\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3250\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3453\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В NER существует сразу несколько типов лэйблов для каждого токена. В случае с conll2003 существуют лэйблы следующих видов:\n",
        "\n",
        "* 'PER' для имен и фамилий\n",
        "* 'ORG' для названия организаций\n",
        "* 'LOC' для локаций\n",
        "* 'MISC' для смешанных сущностей\n",
        "* 'O' для обычных слов\n",
        "\n",
        "Также вначале лэйблов бывают буквы B и I. B означает начало сущности, I необходимо для следующего слова, означающего эту же сущность."
      ],
      "metadata": {
        "id": "tbopuTVhrNcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_list = datasets[\"train\"].features[f\"ner_tags\"].feature.names\n",
        "label_list"
      ],
      "metadata": {
        "id": "Puw88torrOuZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ec1fdbc-2721-443b-ca90-cdcac38faf6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим на пример из датасета:"
      ],
      "metadata": {
        "id": "GH_CB1CvrQvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = datasets[\"train\"][4]\n",
        "print(example.keys())\n",
        "print(example['tokens'])\n",
        "print(example['ner_tags'])"
      ],
      "metadata": {
        "id": "mbR0agG0rSCZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6cd00f0-9aa5-406b-a533-b27ba3d7262b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'])\n",
            "['Germany', \"'s\", 'representative', 'to', 'the', 'European', 'Union', \"'s\", 'veterinary', 'committee', 'Werner', 'Zwingmann', 'said', 'on', 'Wednesday', 'consumers', 'should', 'buy', 'sheepmeat', 'from', 'countries', 'other', 'than', 'Britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.']\n",
            "[5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для каждого отдельного слова есть номер соответствующего лэйбла.\n",
        "\n",
        "Загрузим токенизатор:"
      ],
      "metadata": {
        "id": "zwWmSCXhrUU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "mg3rMBjOrXJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вспомним, что модели семейства BERT используют subword токенизацию, то есть одно слово может получить несколько отдельных токенов."
      ],
      "metadata": {
        "id": "3XHR2C9zrYs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "print(tokens)\n",
        "print(\"Всего слов:\", len(example[\"tokens\"]))"
      ],
      "metadata": {
        "id": "cJil_6w-rZ38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "731627e7-b355-462e-bc6e-b16825e99f6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'germany', \"'\", 's', 'representative', 'to', 'the', 'european', 'union', \"'\", 's', 'veterinary', 'committee', 'werner', 'z', '##wing', '##mann', 'said', 'on', 'wednesday', 'consumers', 'should', 'buy', 'sheep', '##me', '##at', 'from', 'countries', 'other', 'than', 'britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.', '[SEP]']\n",
            "Всего слов: 31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Это означает, что нам необходимо конвертировать лэйблы таким образом, чтобы они соответствовали токенам.\n",
        "\n",
        "Для того, чтобы проверить к какому слову относится тот или иной токен удобно использовать следующую функцию:"
      ],
      "metadata": {
        "id": "Ww9tYQVFrcZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_input.word_ids())"
      ],
      "metadata": {
        "id": "q_tcD3TbrepE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ffccae0-f8a9-404a-bcc4-2e072993c533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 10, 11, 11, 11, 12, 13, 14, 15, 16, 17, 18, 18, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, None]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В исходном тексте было 31 слово, столько же индексов выдал и метод ```word_ids()```"
      ],
      "metadata": {
        "id": "qE0WhfM4rhWU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Написание функции для преобразования лэйблов (3 балла)\n",
        "\n",
        "Ваша задача заключается в том, чтобы написать функцию ```tokenize_and_align_labels()```, которая должна делать токенизацию и преобразовывать лэйблы в формат, соответствующий токенам.\n",
        "\n",
        "То есть:\n",
        "* Если слово получило отдельный токен, то ему соответствует один лэйбл\n",
        "* Если слово получило несколько токенов, то ему должно соответствовать столько же лэйблов. Например, слово crisps получает токенизацию [15594, 2015], тогда в лэйблами для него будет [0, 0]\n",
        "* Если токен является служебным (имеет индекс None при вызове ```word_ids()```), то ему должен соответствовать лэйбл -100. Это специальный индекс, обозначающий те лэйблы, для которых не нужно считать лосс-функцию.\n",
        "\n",
        "Пример:\n",
        "\n",
        "Исходные слова: ```['Only', 'France',\n",
        " 'and',\n",
        " 'Britain',\n",
        " 'backed',\n",
        " 'Fischler',\n",
        " \"'s\",\n",
        " 'proposal',\n",
        " '.']```\n",
        "\n",
        " Исходные лэйблы: ```[0, 5, 0, 5, 0, 1, 0, 0, 0]```\n",
        "\n",
        " После токенизации: ```[101, 2069, 2605, 1998, 3725, 6153, 27424, 2818, 3917, 1005, 1055, 6378, 1012, 102]```\n",
        "\n",
        " Измененные лэйблы: ```[-100, 0, 5, 0, 5, 0, 1, 1, 1, 0, 0, 0, 0, -100]```\n",
        "\n",
        " Также дополнительные примеры можно посмотреть в следующих ячейках ноутбука, которые проверяют корректность реализации функции ```tokenize_and_align_labels()```"
      ],
      "metadata": {
        "id": "aYPGPa7frjJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_all_tokens = True\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "    # Токенизируем текст\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "    labels = []  # В этот массив будем складывать id лэйблов токенов\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "\n",
        "        # Напишите код здесь. Соберите в список label_ids лэйблы, соответствующие токенам\n",
        "\n",
        "        label_ids = []\n",
        "        cur_word = None\n",
        "\n",
        "        for word_id in word_ids:\n",
        "\n",
        "          # Case 1: The start of the new word is met.\n",
        "          # Случай 1: Встречено начало нового слова.\n",
        "\n",
        "          if word_id != cur_word:\n",
        "\n",
        "            cur_word = word_id\n",
        "\n",
        "            # If a special token is met, set its label to -100.\n",
        "            # The reason to use -100 is that during the cross-entropy\n",
        "            # it is the index that is ignored by the loss function.\n",
        "\n",
        "            # Если был встречен специальный токен, присваивается лэйбл\n",
        "            # равный -100. Причина использования -100 заключается в том,\n",
        "            # что во время кросс-энтропии этот индекс игнорируется функцией потерь.\n",
        "\n",
        "            lbl = -100\n",
        "\n",
        "            if word_id != None:\n",
        "              lbl = label[word_id]\n",
        "\n",
        "            label_ids.append(lbl)\n",
        "\n",
        "          # Case 2: A special token is met.\n",
        "          # Случай 2: Встречен специальный токен.\n",
        "\n",
        "          elif word_id == None:\n",
        "\n",
        "            label_ids.append(-100)\n",
        "\n",
        "          # Case 3: The part of the previous word is met.\n",
        "          # Случай 3: Встречена часть предыдущего слова.\n",
        "\n",
        "          else:\n",
        "\n",
        "            # In the case if the label still means that it is \"beginning\n",
        "            # of the word\", change it so that it means \"inside the word\".\n",
        "\n",
        "            # В случае если лэйбл всё ещё означет \"начало слова\", поменять\n",
        "            # его, чтобы он начал означать \"внутри слова\".\n",
        "\n",
        "            # B-... -> I-...\n",
        "\n",
        "            lbl = label[word_id] + 1 if label[word_id] % 2 else label[word_id]\n",
        "            label_ids.append(lbl)\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n"
      ],
      "metadata": {
        "id": "LV0Ktqs0rmNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_examples = {\n",
        "    'id': ['0', '1', '2'],\n",
        "    'tokens': [\n",
        "        ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'],\n",
        "        ['Peter', 'Blackburn'],\n",
        "        ['BRUSSELS', '1996-08-22']\n",
        "        ],\n",
        "    'ner_tags': [\n",
        "        [3, 0, 7, 0, 0, 0, 7, 0, 0],\n",
        "        [1, 2],\n",
        "        [5, 0]\n",
        "        ]\n",
        "    }\n",
        "\n",
        "test_outputs = {\n",
        "    'input_ids': [\n",
        "        [101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102],\n",
        "        [101, 2848, 13934, 102],\n",
        "        [101, 9371, 2727, 1011, 5511, 1011, 2570, 102]\n",
        "        ],\n",
        "    'attention_mask': [\n",
        "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "        [1, 1, 1, 1],\n",
        "        [1, 1, 1, 1, 1, 1, 1, 1]\n",
        "        ],\n",
        "    'labels': [\n",
        "        [-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, -100],\n",
        "        [-100, 1, 2, -100],\n",
        "        [-100, 5, 0, 0, 0, 0, 0, -100]\n",
        "        ]\n",
        "    }"
      ],
      "metadata": {
        "id": "bSJ-LOAsroMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert test_outputs == tokenize_and_align_labels(test_examples), \"Похоже tokenize_and_align_labels работает не так, как должна\""
      ],
      "metadata": {
        "id": "zt3WABYNrpv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Применим функцию ко всем выборкам датасета с помощью метода ```map()```"
      ],
      "metadata": {
        "id": "QexizBVzrslU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)"
      ],
      "metadata": {
        "id": "9To6te8yruGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Тренировака модели\n",
        "\n",
        "\n",
        "Обучать модель будем с помощью ```Trainer``` из библиотеки ```transformers```.\n",
        "\n",
        "Загрузим претренированные веса:"
      ],
      "metadata": {
        "id": "66FvQqPLrwj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"
      ],
      "metadata": {
        "id": "E8SSSLJ_rxz5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c4070af-ff96-4fb5-a986-b46307063e45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Определение аргументов для тренировки (1 балл)\n",
        "\n",
        "Загляните в [документацию](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) и заполните необходимые аргументы для тренировки. Помните, что для файнтюнинга больших моделей следует выбирать небольшой learning rate (обычно меньше 1е-5)."
      ],
      "metadata": {
        "id": "UoGV5bkZr0Rp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell is needed in order for the next one\n",
        "# to work properly. If it still doesn't, restart\n",
        "# the runtime and try again.\n",
        "\n",
        "# Эта ячейка необходима для того, чтобы следующая\n",
        "# работа правильно. Если запустить всё равно не получается,\n",
        "# попробуйте перезагрузить runtime и попробовать снова.\n",
        "\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTytQ1MDy8hF",
        "outputId": "58277950-091c-48fc-d795-ffd9e9feb5d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.21.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (16.0.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "\n",
        "args = TrainingArguments(\n",
        "    # Опишите здесь необходимые аргументы\n",
        "\n",
        "    # The \"evaluation_strategy\" parameter is needed in order to\n",
        "    # identify when the evaluation is done. It is set to \"epoch\"\n",
        "    # which means that evaluation happens at the end of each epoch.\n",
        "\n",
        "    # Параметр \"evaluation_strategy\" необходим для определения того,\n",
        "    # когда происходит evaluation. Выбрано значение \"epoch\", которое означает,\n",
        "    # что evaluation происходит в конце каждой эпохи.\n",
        "\n",
        "    evaluation_strategy = \"epoch\",\n",
        "\n",
        "    # The \"num_train_epochs\" parameter is needed in order to\n",
        "    # set the number of epochs during which the model will train.\n",
        "\n",
        "    # Параметр \"num_train_epochs\" необходим для того, чтобы задать\n",
        "    # количество эпох, во время которых модель будет обучаться.\n",
        "\n",
        "    num_train_epochs = 6,\n",
        "\n",
        "    # The \"learning_rate\" parameter is needed in order to\n",
        "    # set the initial learning rate.\n",
        "\n",
        "    # Параметр \"learning_rate\" необходим для того, чтобы\n",
        "    # задать изначальное значение коэффициента скорости обучения.\n",
        "\n",
        "    learning_rate = 3e-6,\n",
        "\n",
        "    # The parameter \"output_dir\" is needed in order to\n",
        "    # identify the output directory where the model\n",
        "    # predictions and checkpoints will be written.\n",
        "\n",
        "    # Параметр \"output_dir\" необходим для того, чтобы обозначить\n",
        "    # вызодной каталог, где будут описаны предсказания и контрольные\n",
        "    # точки модели.\n",
        "\n",
        "    output_dir = \"./results\"\n",
        ")"
      ],
      "metadata": {
        "id": "rS6w-Tqdr17S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим вспомогательные объекты: ```DataCollatorForTokenClassification``` и ```metric```"
      ],
      "metadata": {
        "id": "ayyZSHrjr3ZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "metric = load_metric(\"seqeval\")"
      ],
      "metadata": {
        "id": "3i1EzFxgr48L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "3c84315669a443cf8fd89ffa076ed0e5",
            "2ec0936c88654b189d5917f2d0e34990",
            "6d2ba3858f8c44dbad440dbb25417872",
            "96711424ca794c5b8e9c481a70b04339",
            "f75fd6383d75475397d808d8fa6b3caa",
            "b07e8cb877d746959a20e8da8dbe0cb1",
            "ad375fccdc134ea09858f56041d63a6f",
            "c9c5c4b340e64528b52a667fd049cf0d",
            "899aca8b3c1f49fbb59835e73746b1fc",
            "ca8e3923afbd4d8495be017fc160996c",
            "b930e7a81b3344c1afc31115336d9414"
          ]
        },
        "outputId": "fb16b019-a17a-429a-ddf2-d3b19ebf51f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-053592a2c97e>:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"seqeval\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/2.47k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c84315669a443cf8fd89ffa076ed0e5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Расчет метрики (1 балл)\n",
        "\n",
        "Опишем функцию ```compute_metric```, которая будет учитывать только нужные токены."
      ],
      "metadata": {
        "id": "Hq75CLiPr7Tb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Changed the passing arguments for compute_metrics().\n",
        "# When it was run in the first place, the error showed\n",
        "# that the funtion did not receive the second parameter.\n",
        "# In order to fix it, one parameter is expected, which is\n",
        "# then separated into two different.\n",
        "\n",
        "# Входные параметры для compute_metrics() были изменены.\n",
        "# Когда программа была запущена в первый раз, ошибка показала,\n",
        "# что функция не получила второй параметр. Чтобы исправить эту\n",
        "# ошибку, был сделан один ожидаемый параметр, который впоследствии\n",
        "# разбивается на два различных.\n",
        "\n",
        "def compute_metrics(ep_predictions):\n",
        "    prds, labels = ep_predictions\n",
        "    predictions = np.argmax(prds, axis=2)\n",
        "\n",
        "    # Удалим из подсчета метрик служебные токены\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    # По аналогии с фильтрацией true_predictions опишите фильтрацию для true_labels\n",
        "\n",
        "    # In order to perform the filtration, the following must be done:\n",
        "    # 1. Remove all the special tokens that were initialized by -100.\n",
        "    # 2. Convert all the integers to the string format (labels).\n",
        "\n",
        "    # Чтобы произвести фильтрацию, необходимо выполнить следующее:\n",
        "    # 1. Убрать все специальные токены, которым были присвоены значения -100.\n",
        "    # 2. Конвертировать все числа в формат строки (лэйблы).\n",
        "\n",
        "    true_labels =  [[label_list[lbl] for lbl in label if lbl != -100] for label in labels] # магия здесь\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "x55dL77Tr8e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Использование ```Trainer``` для обучения (2 балла)\n",
        "\n",
        "Далее создайте объект класса ```Trainer``` с необходимыми аргументами и обучите модель.\n",
        "\n",
        "Подробнее о том, как использовать ```Trainer```, можно почитать [здесь](https://huggingface.co/docs/transformers/main_classes/trainer) или же посмотреть семинарское занятия из этого модуля.  "
      ],
      "metadata": {
        "id": "oOUdZvrhr-MM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создайте объект класса Trainer и обучите модель\n",
        "\n",
        "trainer = Trainer(\n",
        "\n",
        "    # The \"model\" parameter is needed in order to\n",
        "    # declare the model which will be trained,\n",
        "    # evaluated or used for predicitions.\n",
        "\n",
        "    # Параметр \"model\" необходим для того, чтобы обозначить\n",
        "    # модель, которую будут обучать, оценивать или\n",
        "    # использовать для предсказаний.\n",
        "\n",
        "    model = model,\n",
        "\n",
        "    # The \"args\" parameter is needed in order to\n",
        "    # set the set of arguments which will be tweaked\n",
        "    # during the process of training.\n",
        "\n",
        "    # Параметр \"args\" необходим для того, чтобы установить\n",
        "    # набор аргументов, которые будут участвовать в процессе\n",
        "    # обучения.\n",
        "\n",
        "    args = args,\n",
        "\n",
        "    # The \"train_dataset\" parameter is needed in order to\n",
        "    # define the dataset which will be used for training.\n",
        "\n",
        "    # Пармаетр \"train_dataset\" необходим для того, чтобы\n",
        "    # обозначить набор данных, на котором модель будет обучаться.\n",
        "\n",
        "    train_dataset = tokenized_datasets[\"train\"],\n",
        "\n",
        "    # The \"eval_dataset\" parameter is needed in order to\n",
        "    # define the dataset which will be used for evaluation.\n",
        "\n",
        "    # Пармаетр \"eval_dataset\" необходим для того, чтобы\n",
        "    # обозначить набор данных, на котором модель будет оцениваться.\n",
        "\n",
        "    eval_dataset = tokenized_datasets[\"validation\"],\n",
        "\n",
        "    # The \"data_collator\" parameter is needed in order to\n",
        "    # declare the function which will form a batch from a list\n",
        "    # of elements of train_dataset or eval_dataset.\n",
        "\n",
        "    # Параметр \"data_collator\" необходим для того, чтобы обозначить\n",
        "    # функцию, которая сформирует батч из набора элементов из train_dataset\n",
        "    # и eval_dataset.\n",
        "\n",
        "    data_collator = data_collator,\n",
        "\n",
        "    # The \"compute_metrics\" parameter is needed in order to\n",
        "    # declare the function that will be used to compute metrics at evaluation.\n",
        "\n",
        "    # Параметр \"compute_metrics\" необходим для того, чтобы обозначить\n",
        "    # функцию, которая будет использована в расчётах метрик при оценивании.\n",
        "\n",
        "    compute_metrics = compute_metrics,\n",
        "\n",
        "    # The \"tokenizer\" parameter is needed in order to\n",
        "    # identify the operation of how the data will be preprocessed.\n",
        "\n",
        "    # Параметр \"tokenizer\" необходим для того, чтобы обозначить операцию\n",
        "    # того, как данные будут предобработаны.\n",
        "\n",
        "    tokenizer = tokenizer\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "R1BzxvDbr_Zd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "outputId": "8a13c973-6865-4cac-add5-e86a8df86789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='19' max='10536' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   19/10536 00:56 < 9:46:11, 0.30 it/s, Epoch 0.01/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-7bb5dfdebbed>\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m )\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m         )\n\u001b[0;32m-> 1539\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1540\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1809\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2663\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2664\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2665\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2667\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1851\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0munscale_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Получение необходимой метрики (3 балла)\n",
        "\n",
        " Хорошее качество для этой задачи ~0.92 по F1 мере или выше. Попробуйте добиться этого значения, используя различные гиперпараметры в ```TrainingArguments```. Напишите вывод о проделанной работе."
      ],
      "metadata": {
        "id": "wWzf7Ib1sAlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Код\n",
        "\n",
        "# As it can be seen from the information above, the number of epochs\n",
        "# is suitable (as the F1-metric and accuracy don't fall, there is no\n",
        "# overtraining). Let's try to modify the \"learning_rate\" and see what happens.\n",
        "\n",
        "# Как можно увидеть из информации выше, количество эпох подобрано подобающе\n",
        "# (метрика F1 и точность не падают, переобучения не происходит). Попробуем\n",
        "# изменить параметр \"learning_rate\" и посмотреть, что получится.\n",
        "\n",
        "args_mod = TrainingArguments(\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    num_train_epochs = 6,\n",
        "    learning_rate = 1e-5,\n",
        "    output_dir = \"./results\"\n",
        ")\n",
        "\n",
        "trainer_mod = Trainer(\n",
        "    model = model,\n",
        "    args = args_mod,\n",
        "    train_dataset = tokenized_datasets[\"train\"],\n",
        "    eval_dataset = tokenized_datasets[\"validation\"],\n",
        "    data_collator = data_collator,\n",
        "    compute_metrics = compute_metrics,\n",
        "    tokenizer = tokenizer\n",
        ")\n",
        "\n",
        "trainer_mod.train()"
      ],
      "metadata": {
        "id": "sxZ0IZ38sBu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now it can be seen that F1-metric has improved. Nevertheless, the accuracy\n",
        "# has fallen on the sixth epoch. There is an overtraining, so the number of\n",
        "# epochs must be decreased (for these parameters it will be decreased by one).\n",
        "\n",
        "# Теперь можно увидеть, что метрика F1 улучшилась. Тем не менее точность упала\n",
        "# на шестой эпохе. Происходит переобучение, поэтому количество эпох должно\n",
        "# быть уменьшено (для данных параметров оно будет уменьшено на одно).\n",
        "\n",
        "args_fin = TrainingArguments(\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    num_train_epochs = 5,\n",
        "    learning_rate = 1e-5,\n",
        "    output_dir = \"./results\"\n",
        ")\n",
        "\n",
        "trainer_fin = Trainer(\n",
        "    model = model,\n",
        "    args = args_fin,\n",
        "    train_dataset = tokenized_datasets[\"train\"],\n",
        "    eval_dataset = tokenized_datasets[\"validation\"],\n",
        "    data_collator = data_collator,\n",
        "    compute_metrics = compute_metrics,\n",
        "    tokenizer = tokenizer\n",
        ")\n",
        "\n",
        "trainer_fin.train()"
      ],
      "metadata": {
        "id": "Lq3LdTLmQEJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### At the end it can be concluded that even though throught the training metrics fluctuated, the overall result has improved.\n",
        "\n",
        "### В конечном итоге можно сделать вывод, что несмотря на то что в процессе обучения показатели метрик колебались, конечный результат улучшился."
      ],
      "metadata": {
        "id": "8HZTR6DleG1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "# Вывод"
      ],
      "metadata": {
        "id": "qeloL2Tffjoi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This work has helped me to understand better what is the \"Token classification\" task about and gave me a hands on experience with NER, how different aspects for this problem are tuned. Steps which were implemented during the work on this notebook were:\n",
        "\n",
        "* Studying the problem\n",
        "* Studying the given dataset\n",
        "* Data preparation and procession\n",
        "* Metric calculation\n",
        "* Adjusting needed parameters and tuning the model\n",
        "* Training the model\n",
        "\n",
        "### Even though the first metrics that were obtained with the initial \"args\" are not qualified as \"good quality\", the research on how to improve them has also helped to understand that the overall performance of the model depends heavily not only on the programming skills of the developer but also on the ability to understand and analyse the task from the mathematical point of view."
      ],
      "metadata": {
        "id": "HRR5pBW3bGPh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Эта работа позволила мне лучше понять в чём заключается задача классификации токенов и дала мне опыт работы с NER, каким образом настраиваются разные аспекты этой задачи. Шаги, которые были выполнены в процессе работы, включают в себя:\n",
        "\n",
        "* Изучение поставленной проблемы\n",
        "* Изучение набора данных\n",
        "* Подготовка и обработка данных\n",
        "* Подсчёт метрики\n",
        "* Выбор нужных параметров и настройка модели\n",
        "* Тренировка модели\n",
        "\n",
        "### Несмотря на то что первые полученные метрики с изначальными параметрами \"args\" не квалифицируются как хорошее качество, исследование о том, как их исправить, также позволило осознать, что общие показатели модели сильно зависят не только от навыков программирования разработчика, но также от способности понять и проанализировать задачу с математическо точки зрения."
      ],
      "metadata": {
        "id": "humw0HGkfpk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Дополнительный эксперимент (2 балла)\n",
        "\n",
        "А теперь попробуйте решить ту же задачу, но с другой претренированной моделью из семейства BERT, например, ```roberta-base``` или ```distillroberta-base``` и получить качество выше 0.94 по F1 на валидационном датасете. Список доступных моделей можно посмотреть [здесь](https://huggingface.co/models). Вы на практике убедитесь, насколько различные претренированные модели могут улучшать конечное качество на downstream задачах.\n",
        "\n",
        "Для выполнения этого пункта можно всего лишь скопировать некоторые ячейки кода выше и поменять переменную ```model_checkpoint``` на название другой модели."
      ],
      "metadata": {
        "id": "l63ePuqWsDfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Проведите эксперимент здесь\n",
        "\n",
        "# The following realisation is done for \"roberta-base\" model.\n",
        "# Because of the limited resources, the following example shows\n",
        "# the performance around 0,948 on F1-metric. The parameters can be adjusted,\n",
        "# so that above 0,95 is reached. I tried 5 epochs and 1e-5 learning rate, which\n",
        "# resulted in above 0,95 score on F1-metric. Nevertheless, the overtraining was\n",
        "# discovered. To sum up, the parameters given in this example help to obtain\n",
        "# the score above 0,94, however the result can be improved for which\n",
        "# additional resources are required for Google Colab.\n",
        "# (repeated training for checking different parameters)\n",
        "\n",
        "# Следующая реализация выполнена для модели \"roberta-base\".\n",
        "# Из-за ограниченных ресурсов, приведенный пример показывает около 0,948\n",
        "# по шкале F1. Параметры могут быть подобраны таким образом, что показатели\n",
        "# выше 0,95 достигаются. Я пробовал 5 эпох и коэффициент скорости обучения\n",
        "# равный 1e-5, которые помогли получить результат выше 0,95 по шкале F1.\n",
        "# Тем не менее, было обнаружено переобучение. В итоге данные параметры\n",
        "# помогают достичь показатель выше 0,94, однако результат может быть\n",
        "# улучшен для чего понадобится больше ресурсов Google Colab.\n",
        "# (повторное обучение для проверки различных параметров)\n",
        "\n",
        "model_exp = \"roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_exp, add_prefix_space=True)\n",
        "tokenized_datasets_experiment = datasets.map(tokenize_and_align_labels, batched=True)\n",
        "model_experiment = AutoModelForTokenClassification.from_pretrained(model_exp, num_labels=len(label_list))\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "args_experiment = TrainingArguments(\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    num_train_epochs = 4,\n",
        "    learning_rate = 5e-6,\n",
        "    output_dir = \"./results\"\n",
        ")\n",
        "\n",
        "trainer_experiment = Trainer(\n",
        "    model = model_experiment,\n",
        "    args = args_experiment,\n",
        "    train_dataset = tokenized_datasets_experiment[\"train\"],\n",
        "    eval_dataset = tokenized_datasets_experiment[\"validation\"],\n",
        "    data_collator = data_collator,\n",
        "    compute_metrics = compute_metrics,\n",
        "    tokenizer = tokenizer\n",
        ")\n",
        "\n",
        "trainer_experiment.train()"
      ],
      "metadata": {
        "id": "npHC4DwPyxal"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}